## TCP/IP

TCP监控：`tcpdump -nn -i eth0 port 端口号`，用来抓取数据包

当服务端程序启动，`netstat -natp` 可看到：服务端会开启LISTEN状态申请端口号，监听端口号

然后`lsof -p pid` 查看进程文件描述符的分配情况



#### TCP连接

TCP协议是面向连接的，走完三次握手，双方就会有资源的开辟，尽管服务端程序阻塞住无法接收客户端发的包，客户端内核也会开辟资源进行接收和等待

双方通过三次握手开辟资源，可为对方提供服务，连接就建立了，连接不是物理的，看不见摸不着，是靠资源来代表的



#### Socket连接四元组

ClientIP + ClientPort + ServerIP + ServerPort

至少有一个不同，才可以**唯一**区分一个连接，只要满足**唯一性**，连接就可以被创建，通信就可以被允许

即两个端口之间只能存在一个连接

服务端只需监听自己的一个端口号即可，不需要再为客户端连接重新分配一个端口号

每个四元组都会被内核抽象为一个FD来代表，服务端客户端都是如此。且因为FD是每个进程独有的，进程间又相互隔离，故不同进程拿到的FD可能会重名

```shell
ClientIP + APort + ServerIP + Aport = FD3
ClientIP + BPort + ServerIP + Aport = FD4
ClientIP + CPort + ServerIP + Aport = FD5

ClientIP + APort + ServerIP + Bport = FD3
ClientIP + BPort + ServerIP + Bport = FD4
ClientIP + CPort + ServerIP + Bport = FD5
```

accept之后，接收内核这个连接的FD，在Java中就被包装为一个Socket对象

端口不能共用，每个网络进程都要LISTEN状态监听端口，共用同一端口就区分不出数据包要给哪个进程了，所以会有端口占用的问题

#### 数据包

MTU：数据包大小

MSS：真实数据大小

数据包是先放在**内核缓冲区**里，然后被进程处理

若数据太大，则切割成若干个MSS，拼成多个MTU

TCP拥塞：既要提高性能，也不能发爆了，发太多是会直接丢弃的，丢弃是丢后面的包，但是可配置

看到09完，tcp拥塞，要回去08画一下服务端客户端的连接示意图，内核缓冲区啥的



任何IO模型，上层应用和内核之间有固定的三步：

- 系统调用返回socket文件描述符
- 绑定端口
- 监听文件描述符

accept，从FD等待客户端连接，进入阻塞状态

recv，从FD读取客户端传来的信息，进入阻塞状态

#### BIO式

主线程死循环接收accept，随后克隆多个线程，来一个连接起一个线程，每个的线程做读取操作，recv

内核while死循环监听，连接进来，进行accept系统调用，随后fork线程，每次都创建线程，所以速度慢

无论哪种IO模型，都必须三次握手

BIO的弊端：阻塞(blocking)，accept阻塞，读取也阻塞































linux为什么慢看到0006