## TCP/IP

TCP监控：`tcpdump -nn -i eth0 port 端口号`，用来抓取数据包

当服务端程序启动，`netstat -natp` 可看到：服务端会开启LISTEN状态申请端口号，监听端口号

然后`lsof -p pid` 查看进程文件描述符的分配情况



#### TCP连接

TCP协议是面向连接的，走完三次握手，双方就会有资源的开辟，尽管服务端程序阻塞住无法接收客户端发的包，客户端内核也会开辟资源进行接收和等待

双方通过三次握手开辟资源，可为对方提供服务，连接就建立了，连接不是物理的，看不见摸不着，是靠资源来代表的



#### Socket连接四元组

ClientIP + ClientPort + ServerIP + ServerPort

至少有一个不同，才可以**唯一**区分一个连接，只要满足**唯一性**，连接就可以被创建，通信就可以被允许

即两个端口之间只能存在一个连接

服务端只需监听自己的一个端口号即可，不需要再为客户端连接重新分配一个端口号

每个四元组都会被内核抽象为一个FD来代表，服务端客户端都是如此。且因为FD是每个进程独有的，进程间又相互隔离，故不同进程拿到的FD可能会重名

```shell
ClientIP + APort + ServerIP + Aport = FD3
ClientIP + BPort + ServerIP + Aport = FD4
ClientIP + CPort + ServerIP + Aport = FD5

ClientIP + APort + ServerIP + Bport = FD3
ClientIP + BPort + ServerIP + Bport = FD4
ClientIP + CPort + ServerIP + Bport = FD5
```

accept之后，接收内核这个连接的FD，在Java中就被包装为一个Socket对象

端口不能共用，每个网络进程都要LISTEN状态监听端口，共用同一端口就区分不出数据包要给哪个进程了，所以会有端口占用的问题

<img src=".\pic\内核网络连接原理图.png" style="zoom: 80%; float:left" />

#### 数据包

MTU：数据包大小

MSS：真实数据大小

数据包是先放在**内核缓冲区**里，然后被进程处理

若数据太大，则切割成若干个MSS，拼成多个MTU



#### TCP拥塞

既要提高性能，也不能发爆了，发太多是会直接丢弃的，丢弃是丢后面的包，但是可配置

若窗口被填满了，三次握手回的那个ack会告诉客户端没有余量了，此时客户端就可以阻塞不发了

服务端阻塞不读FD，客户端一直给他发，数据包就一直放在**内核缓冲区**，可以在`netstat -natp`看到对应进程的Recv-Q数一直增加，增加到一定数量就会丢掉后面发的包了

（一）09

`自己做实验Recv-Q总是1，没有被撑满`

**对照博客学一下TCP发送/接收缓冲区、滑动窗口、TCP拥塞**



#### TCP参数

TCP_NODELAY：是否延迟优化（指数据包攒够一定大小的包再发包，而不是输入多少立马就发。nodelay就是不做延迟优化处理）

BACK_LOG：服务端线程跑满时，队列中最大等待任务数

CLI_KEEPALIVE：是否开启心跳，通过发送空包来判断对方是否活着，是否长连接

CLI_LINGER：是否立刻结束，释放端口

CLI_TIMEOUT：客户端读取超时时间



#### BIO式

- `strace -ff -o out /java MainClass` 追踪每个线程对内核的系统调用
- 线程fork和clone的区别：fork是本质，clone是基于fork之上加了参数的一层包装

- 任何IO模型，上层应用和内核之间有固定的三步：
  - socket = fd3：系统调用返回socket文件描述符
  - bind(fd3,8080)：绑定端口
  - listen(fd3)：监听文件描述符

  完事就可以在netstat -natp看到四元组  

- accept，进入阻塞状态，从FD等待客户端连接

- recv，进入阻塞状态，从FD读取客户端传来的信息

- ServerSocket是服务端的，Socket是客户端的

- 主线程死循环接收accept，随后克隆多个线程，来一个连接起一个线程，每个的线程做读取操作，recv

- 内核while死循环监听，连接进来，进行accept系统调用，随后fork线程，每次都创建线程，所以速度慢

- 无论哪种IO模型，都必须三次握手

- BIO的弊端：阻塞(blocking)，accept阻塞，读取也阻塞

- BIO因为阻塞，所以抛线程，才能满足一个服务端处理多个客户端 



#### Epoll/NIO

select：有限制1024个客户端channel连接，循环遍历每个请求看有没有数据

poll：无限制，循环遍历每个请求看有没有数据

epoll：无限制，直接在事件就绪列表中获取有数据的请求

epoll是由Linux三个函数实现：epoll_create（创建epoll实例，一个C语言结构体）, epoll_ctl（监听channel注册过来的事件）, epoll_wait

epoll原理：当socket接收到数据时，中断程序调用回调函数会给epoll实例的事件就绪列表rdlist里添加该socket的引用（由操作系统中断实现），当程序执行到epoll_wait时，若rdlist已经引用了socket，那epoll_wait直接返回，若rdlist为空则阻塞进程

中断：系统用来响应硬件设备请求的一种机制，OS收到硬件的中断请求时，会打断正在执行的进程，调用内核中的中断处理程序来响应请求



#### NIO

nio是内核中增加的函数

用一个线程处理接收连接和接收数据，两个过程都可以设置为阻塞/非阻塞

死循环监听连接，调用accept()方法（调内核的操作）**设置为不阻塞**，没连接返回-1/NULL，有连接返回FD，对应Java中的SocketChannel，并把连接放进链表中，继续遍历链表，取出每个连接，这里也**设置为不阻塞**，调用read()方法（也是系统调用，用户态切换到内核态），尝试读取，有东西就进行数据**读写**，没东西就遍历下一个连接，全部连接处理完就进入下一次接受连接accept()，然后继续遍历链表，循环往复

<img src=".\pic\NIO模型.png" style="zoom: 80%; float:left" />



#### 多路复用

<img src=".\pic\多路复用模型.png" style="zoom:80%; float:left" />





#### IO模型

同步阻塞：程序自己读取，调用方法，没读到就一直等

同步非阻塞：程序自己读，调用方法，读到就处理，没读到就走，下回再来读他

异步：异步要内核主动通知程序可以读取了，但Linux没有通用内核级的处理方案，因为不安全，会让内核做的事情太多，Windows就有异步



#### 多路复用器

select、poll、epoll都是同步下的非阻塞模型的多路复用器



NIO、select、poll都要遍历所有FD，都要调内核去询问状态



NIO没用多路复用器，遍历过程的成本在用户态内核态切换

select/poll只触发一次用户态内核态切换，把所有FD一次性传递给内核，由内核去遍历FD，修改状态

由于内核不保存应用的域，内核不知道应用要获取哪些FD的状态，所以每次select/poll调用内核，给内核发送FD的列表，内核都得遍历所有FD一次

因为select/poll遍历的操作发生在内核，而NIO的遍历一次是每询问一次都发起一次内核调用，所以select/poll的效率比NIO高





#### select/poll劣势

每次都要重复传递所有FD给内核，内核就需要开辟空间，触发一次全量遍历FD集合



#### IO中断处理

中断有个中断向量表，一个字节代表255个中断，这里是触发80中断指令

数据到网卡，然后到内存，然后触发IO中断，随后进行callback响应，处理回调逻辑（这里就是看成事件），再将数据关联到FD（buffer）

把select的遍历打散到事件到达的时候



#### Epoll

在中断的时候到红黑树中找到FD，再把FD拷贝到链表中

针对每次都要重复传递所有FD给内核，Epoll选择在内核开辟空间

第一件事情，调用epoll_create()，返回epfd（Epoll FD） ，此时在内核开辟空间，里面放红黑树

然后调用epoll_ctl，将listen的fd4（四元组fd）添加进红黑树中

下一步调用epoll_wait，等待链表，从链表中获取fd的数据

数据到达网卡，放到buffer中。此时Epoll在IO中断中做了延伸：从红黑树找到这个fd4，再将fd4**复制**到链表中，这个过程就是对应IO中断处理

epoll_wait()时没有发生全量fd的遍历，就是因为内核Epoll对IO中断处理的延伸操作



Socket中包含了发送缓冲区、接收缓冲区、等待队列等

