## 用户空间和内核空间

#### 内核空间（Kernel space）

是OS内核的运行空间（OS内核可简单理解为OS），OS本质也是比较底层的软件，本身运行也需要内存空间，所以OS使用的这块空间就叫内核空间

内核空间是内核代码运行的地方

进程运行在内核空间就处于内核态

内核空间可以执行任意命令，调用系统一切资源，比如磁盘写入操作、使用Socket通过网络往外发送数据（网卡也是系统资源）

#### 用户空间（User space）

Redis、Nginx等上层应用运行时用的内存就是用户空间

用户空间是用户程序代码运行的地方

进程运行在用户空间就处于用户态

用户空间只能执行简单运算，不能直接调用系统资源，必须通过系统调用接口（system call），才能向内核发出指令

- 每个程序都是线性的内存地址

 

> 在内存的使用上，如果上层应用运行时占据了OS的运行空间，就会导致OS运行出问题，所以为了安全，要隔离两个空间，即使用户程序崩溃了，内核也不受影响	







##  磁盘和内存间数据传输方式

#### PIO

早期使用PIO实现，读取磁盘文件到内存中，数据需要经过CPU存储转发，需要占用大量CPU时间来读取文件，导致访问文件时系统几乎停止响应

#### DMA（直接内存访问  Direct Memory Access）

不经过CPU，直接进行磁盘和内存（内核空间）的数据交换

CPU只需要向DMA控制器下达指令，让其处理数据传输，DMA通过系统总线来传输数据，传送完毕再通知CPU去内存取数据，很大程度降低CPU占有率，大大节省系统资源







## 缓存IO和直接IO

#### 缓存IO（Linux默认）

数据从磁盘先通过DMA复制到内核空间，内核空间起到一个缓冲作用，再从内核空间通过CPU复制到用户空间

优点：减少IO读写次数，一次系统调用，全部数据直接落盘

缺点：增加数据副本，耗时 

#### 直接IO

数据直接从磁盘通过DMA复制到用户空间，**Mysql**使用直接IO，数据直接落盘，硬盘IO较多







## IO访问方式

#### 磁盘IO

当应用程序调用read接口时，OS先检查数据在不在内核的高速缓存，有就直接在缓存中返回，没有就从磁盘读取，然后保存在OS的缓存中

当应用程序调用write接口时，数据从用户空间复制到内核空间的缓存中，对应用程序来说，写操作已经完成，由OS决定何时将数据写入磁盘，除非显式调用sync同步命令

#### 网络IO

- 当调用read系统调用时，通过DMA将数据复制到内核
- 然后由CPU控制将内核数据复制到用户空间下的buffer中
- read调用完成后，write调用首先将用户空间buffer中的数据复制到内核空间下的socket buffer中
- 最后通过DMA复制将内核空间下的socket buffer中的数据复制到网卡中传输

#### 磁盘IO与网络IO对比

- 磁盘IO主要由转动、寻址、块传输延时决定
- 网络IO主要由服务器响应延时、带宽限制、网络延时、跳转路由延时、本地接收延时决定，受环境干扰极大

所以能磁盘IO就不要网络IO







## 操作系统相关

- 虚拟文件系统（VFS）

  Linux/类Unix系统内核空间有一个虚拟文件系统（VFS树），是个目录树，树上不同的结点可以映射到不同的物理位置，每个物理位置可以是不同的文件系统

  解决上层应用程序对计算机不同存数据的硬件（驱动不同、实现形式不同）的数据的访问

  VFS可类比于JDBC在Java程序和DB之间的关系，一个JDBC访问不同的数据库，是一个解耦层。VFS是给用户空间程序暴露的访问硬盘数据的统一接口，下面挂载了不同的设备，各种设备的实现和驱动不相同

- inode（就是id）

  在打开每个磁盘文件时，都会有一个唯一的inode代表他，<u>inode对应的内存中，</u>打开文件时，会开辟一个pagecache（默认4k) ，若两个程序打开同一个文件，会共享一份pagecache

  ```
  inode和pagecache的关系？
  ```

- 程序和硬盘之间隔了个内核，内核通过pagecache，来维护程序想读的数据。

  若程序修改了数据，内核会给对应pagecache标识为dirty，会有一个flush的过程，写进磁盘，写的形式就决定了**IO模型**。

  dirty标识是内核对所有上层打开的文件的统一管理，内核会在一定条件下整体把所有脏页flush到磁盘中，可以设定内核的调优参数（flush的阈值）来调整，比如达到多少物理内存就flush一次，或多长时间flush一次

  一种是等内核flush，一种是程序调用内核直接刷

- 文件类型

  -：普通文件（可执行、图片、文本）

  d：目录

  l：链接

  b：块设备

  c：字符设备

  s：socket

  p：pineline

  


#### 文件描述符（FD）

- seek，每个进程访问文件时都有自己独立的fd，每个fd里都有一个指针seek，维护进程在读取同一个文件（page）时的不同位置，有自己的seek指针标识自己该读到哪个偏移量

- lsof -p 进程id（$$代表当前bash进程id）

  列出进程打开的文件

- FD记录了对应文件当前读取的偏移量、元信息啥的。保存了对所打开文件的描述，提供给当前进程使用

- 任何程序都有012这三种FD

  0：标准输入 -- sysyem.in

  1：标准输出 -- sysyem.out

  2：报错输出 -- sysyem.error

- 两个程序可以打开同一个文件，但FD会各自维护各自的指针，文件的数据在内存里是同一份的

  内核为每一个进程各自维护了一套数据，数据里包含进程的FD，FD维护了关于FD指向的文件的偏移量、inode、元数据信息

- 两个bash进程打开同一个文件，会产生两个FD，分别记录各自进程打开的文件信息

  各自的seek会指向各自进程在文件的当前偏移量，即读取到文件的哪个位置

  且两个进程不互相影响，两个FD记录的偏移量可以不相同

```shell
bash1进程：
#将文件描述符指向文件file
exec fd1< file  
#列出当前bash1进程打开的文件，可以看到文件描述符fd1代表的文件偏移量为0
lsof -op $$
#输入流读取fd1文件（有换行符就只能读一行）给变量a
read a <& fd1
#再次lsof，可以看到fd1代表的文件偏移量发生了变化
lsof -op $$

bash2进程：
#另开一个bash2进程，对同一文件重复以上打开和读取的过程，lsof查看文件的文件描述符，对比可以看到文件在bash1和bash2的偏移量不同，不互相影响
```



#### 内核映射目录（/proc）

把内核的变量属性、以及进程，以文件的形式挂载在该目录下，在Linux中**一切皆文件！**

这是VFS，目录并非一定是磁盘上的 

cd /proc/$$ 或 cd /proc/$BASHPID：进入当前bash进程的目录下

$$ 和 $BASHPID 表示当前bash进程的pid

该目录下的fd目录，保存了当前进程所有的FD，也可以用lsof -op $$查看当前进程所有FD的细节，如偏移量和指针等
还可以看到当前进程开启的io

```shell
lrwx------ 1 root root 64 Apr 14 17:24 0 -> /dev/pts/0
lrwx------ 1 root root 64 Apr 14 17:24 1 -> /dev/pts/0
lrwx------ 1 root root 64 Apr 14 17:23 2 -> /dev/pts/0
lrwx------ 1 root root 64 Apr 14 17:24 255 -> /dev/pts/0
lrwx------ 1 root root 64 Apr 14 18:55 8 -> socket:[137756658]
#最后一个看到的是文件，实际上是个socket，用FD去描述他
```

 

#### 重定向操作

- 重定向，改变io指向的文件，是一种机制，不是命令

- 重定向操作符

  输入：<	例如：ls ./ 0<

  输出：>	例如：ls ./ 1>

- ls程序的输出（FD为1）就是输出到屏幕

  ```shell
  #把ls程序的本该输出到屏幕的结果重定向输出到ls.out文件中
  ls ./ 1> ls.out
  ```

- cat ooxx.txt
  cat程序用输入流读取文件，用输出流输出到屏幕

  ```shell
  #把cat读取到的文件内容重定向输出到cat.out文件中
  cat 0< ooxx.txt 1> cat.out
  ```

- read a [等待键盘输入]

  ```shell
  #将输入从等待键盘输入重定向到cat.out
  read a 0< cat.out
  #这时变量a只会输出cat.out文件的第一行内容，因为io流对换行符很敏感，一读到换行符就结束了
  echo $a
  ```
- 重定向操作符

  左边跟的是要改变的程序它自有的FD，且两者之间不能有空格
一般右边放的是文件，但是可以把重定向操作符对接起来，即右边放FD，把一个流重定向指向另一个流，但要求重定向操作符后面跟一个&符

  ```shell
  #asfsd是某个不存在的文件，会输出报错
  #先让1(输出)指向ls.out文件，再让2(即报错输出)指向1
  #即实现把1的内容输出到ls.out文件，把2的报错内容传递到1，也追加输出到ls.out文件
  ls ./ /asfsd 1> ls.out 2>& 1
  ```

  

#### 管道

- head/tail -n file

  输出文件头/尾第n行，若不指定n，默认输出头/尾10行

- 管道（|）

  前面的输出作为后面的输入

  ```shell
  #输出文件头8行的最后1行（即输出文件第8行）
  head -8 test.txt | tail -1	
  ```

- 父子进程

  在当前进程/bin/bash进入子进程的bash，pstree查看进程树

  若在父进程定义变量x并赋值，子进程无法访问该变量，需在定义时在x前面加export，使其子孙进程都可访问该变量，这就是在/etc/profile文件中配置环境变量时需要加export的原因

- 指令块

  { echo "aaa"; echo "bbb"; }

  花括号内的指令在同一进程执行  

  ```shell
  #管道符两端会起两个子线程执行命令
  { echo $BASHPID; read x; } | { cat;  }
  ```




#### Page

- OS管理物理内存线性地址时，内核会把内存分为一个个4k的页，按页给程序分配内存，程序随用随申请，不会全量分配

- 程序运行时，它看自己是虚拟的线性地址（虚拟内存），会映射到不连续的物理内存地址。

  程序申请物理内存空间时，内核会分配一页或者多个连续的页给程序，CPU的**MMU**（内存管理单元）把虚拟内存地址转换及映射到物理page（默认4k）

- 当程序要使用80这块虚拟内存，但还没映射到某个物理内存地址时，即一开始分配的内存不够了，就会产生缺页异常，此时会从用户态切换到内核态，内核再从物理内存分配一个page给虚拟内存映射。再从内核态切回用户态，就可以继续执行访问80虚拟内存了

- 操作系统把程序在进程中分为数据段和代码段

- page是存数据的硬件设备和运行的进程之间的抽象层



#### Page Cache

- Page Cache是内核维护的一个中间层，用来优化IO性能

  优先使用内存，减少硬件IO调用

  但由于在程序和磁盘间挡了一层，会产生一致性和可靠性的问题
  
  若Page Cache刷进磁盘不及时，断电就会丢数据
  
  **OS没有绝对的数据可靠性**
  
  即便为了可靠性，调成最慢的方式，单点问题也会严重损耗性能。解决方式：主从复制，主备HA

两个进程访问同一份数据，是使用OS维护的同一份page，这是OS的内存优化。否则每个进程都得到磁盘把数据加载到内存 

- Page Cache上面一层才是所谓的程序IO

- Page Cache和page不是一个东西

- **06-0020** 内存写进磁盘的阈值，关联到redis持久化（aop），mysql调优binlog、unlog、relog级别。

  ```shell
  # 内核脏页参数
  sysctl -a | grep dirty
  
  vm.dirty_background_bytes = 0
  #后台阈值，达到阈值在后台把脏页写进磁盘，再写则触发LRU
  vm.dirty_background_ratio = 10  
  vm.dirty_bytes = 0
  # 前台阈值，达到阈值把脏页阻塞写进磁盘，程序不能往内存写了，再写则触发LRU
  vm.dirty_ratio = 30  
  # 脏页存活时长
  vm.dirty_expire_centisecs = 3000
  vm.dirty_writeback_centisecs = 500
  
  ```

- Java尽量使用Buffer流来进行IO操作，是应用缓冲来减少系统调用（write函数）的损耗

  ```java
  public void aa() throws Exception {
          File file = new File("");
  //        FileOutputStream out = new FileOutputStream(file);
  //        BufferedOutputStream out = new BufferedOutputStream(new FileOutputStream(file));
          while (true) {
              out.write(1);
          }
      }
  ```

- 以前需要out.write()进行系统调用，有用户态内核态的切换，才能让程序的数据到达内核pagecache

  现在使用MappedByteBuffer.put()，不进行系统调用也可以让数据进入pagecache

  但mmap的内存映射，依然受内核的pagecache体系所约束



#### 直接IO

忽略Linux的pagecache，通过DMA直接交给程序自己开辟的一个字节数组（当做pagecache）里面去

程序独享一份pagecache，在系统资源没其他程序用，或用的不多的情况下，这样可以更加细粒度地控制系统IO，来提升自己程序的性能。（如果直接修改系统内存写进磁盘的阈值，是全局的修改，会影响到所有程序）

但需要程序自己维护一致性、dirty等一系列复杂问题

DB一般会使用直接IO
